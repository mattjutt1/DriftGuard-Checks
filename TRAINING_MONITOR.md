# ğŸš€ PromptWizard Training Monitor

**Space URL**: <https://huggingface.co/spaces/unfiltrdfreedom/promptwizard-qwen-training>
**Status**: ğŸ”¨ Building...

## What to Expect When Ready

### Gradio Interface Layout

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ§™ PromptWizard Qwen Fine-tuning  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model: [Qwen/Qwen2.5-7B        â–¼]  â”‚
â”‚ Epochs: [====3====]                â”‚
â”‚ Batch Size: [====4====]            â”‚
â”‚ Learning Rate: [2e-5]              â”‚
â”‚ â–¡ Use Weights & Biases             â”‚
â”‚                                     â”‚
â”‚ [ğŸš€ Start Training]                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Training Output:                   â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚ â”‚ GPU Status: T4, 15GB available  â”‚ â”‚
â”‚ â”‚ Loading GSM8K dataset...         â”‚ â”‚
â”‚ â”‚ Train samples: 100               â”‚ â”‚
â”‚ â”‚ Eval samples: 50                 â”‚ â”‚
â”‚ â”‚ Starting training...             â”‚ â”‚
â”‚ â”‚ Epoch 1/3: â–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘ 45%         â”‚ â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Training Stages

### Stage 1: Initialization (2-3 min)

- Load Qwen model with LoRA
- Configure 8-bit quantization
- Load GSM8K dataset

### Stage 2: Training (30-60 min)

- Process 100 training examples
- Validate on 50 test examples
- Update model weights
- Show loss metrics

### Stage 3: Model Upload

- Save to HuggingFace Hub
- Model available at: `unfiltrdfreedom/promptwizard-qwen-gsm8k`

## Real Metrics We'll See

- **Perplexity**: Lower is better (target: <10)
- **Loss**: Should decrease over epochs
- **Learning Rate**: Follows cosine schedule
- **GPU Memory**: ~10-12GB usage with 8-bit

## Commands While Training

### Monitor GPU Usage

The interface will show real-time GPU metrics

### Expected Timeline

- Building: 2-3 minutes â¬…ï¸ We are here
- Interface Ready: 1 minute
- Training Start: User triggered
- Training Time: 30-60 minutes
- Model Upload: 2-3 minutes

## Success Indicators

âœ… Loss decreasing steadily
âœ… No out-of-memory errors
âœ… Validation loss not increasing (no overfitting)
âœ… Model successfully pushed to Hub

---
Monitoring started at: 23:11:30
