# PromptEvolver 3.0 Training Configuration
# =========================================
# Comprehensive training configuration for Qwen3:4b fine-tuning
# Using QLoRA optimization and latest 2025 best practices
#
# Copyright (c) 2025 Matthew J. Utt

# Model Configuration
model:
  name: Qwen/Qwen2.5-3B    # Latest Qwen 2.5 3B model (more capable than old 4B)
  revision: main
  trust_remote_code: true
  torch_dtype: auto    # Will use float16 for QLoRA

  # Alternative models (uncomment to use)
  # name: "Qwen/Qwen2.5-1.5B"  # Smaller, faster training
  # name: "Qwen/Qwen2.5-7B"    # Larger, better quality (needs more VRAM)

# Quantization Configuration (QLoRA)
quantization:
  enabled: true
  load_in_4bit: true
  bnb_4bit_compute_dtype: float16    # or "bfloat16" for newer GPUs
  bnb_4bit_quant_type: nf4    # Normal Float 4-bit (best for QLoRA)
  bnb_4bit_use_double_quant: true  # Double quantization for extra memory savings

# LoRA Configuration
lora:
  # Rank - higher = better quality, more memory
  r: 16  # Good balance (can use 8 for faster, 32 for better quality)

  # Alpha - typically 2x rank
  lora_alpha: 32

  # Dropout for regularization
  lora_dropout: 0.05

  # Bias handling
  bias: none    # Can be "none", "all", or "lora_only"

  # Target modules - all linear layers for best results
  target_modules:
  - q_proj          # Query projection
  - k_proj          # Key projection
  - v_proj          # Value projection
  - o_proj          # Output projection
  - gate_proj       # Gate projection (MLP)
  - up_proj         # Up projection (MLP)
  - down_proj       # Down projection (MLP)
    # - "lm_head"   # Uncomment to also train output layer (more memory)

  # Task type
  task_type: CAUSAL_LM

  # Additional LoRA settings
  inference_mode: false  # Set to false for training
  modules_to_save: []    # Additional modules to save (e.g., ["lm_head"])

# Training Arguments
training:
  # Output directory
  output_dir: ./models/qwen-promptevolver

  # Number of training epochs
  num_train_epochs: 3

  # Batch sizes (adjust based on GPU memory)
  per_device_train_batch_size: 4  # Reduce if OOM
  per_device_eval_batch_size: 4

  # Gradient accumulation for effective larger batch size
  gradient_accumulation_steps: 4  # Effective batch = 4 * 4 = 16

  # Learning rate and scheduler
  learning_rate: 2.0e-4  # Good starting point for LoRA
  lr_scheduler_type: cosine    # or "linear", "constant"
  warmup_ratio: 0.03  # 3% of steps for warmup

  # Weight decay for regularization
  weight_decay: 0.001

  # Gradient clipping
  max_grad_norm: 0.3

  # Optimizer
  optim: paged_adamw_32bit    # Memory-efficient optimizer
  # optim: "adamw_torch"  # Standard AdamW
  # optim: "adamw_8bit"   # 8-bit AdamW for more memory savings

  # Mixed precision training
  fp16: true  # Use if GPU supports (older GPUs)
  bf16: false  # Use if GPU supports (newer GPUs like A100)
  tf32: true  # Enable TF32 on Ampere GPUs

  # Gradient checkpointing for memory efficiency
  gradient_checkpointing: true
  gradient_checkpointing_kwargs:
    use_reentrant: false  # More memory efficient

  # Logging and saving
  logging_dir: ./logs
  logging_steps: 10
  logging_first_step: true
  save_strategy: steps    # or "epoch"
  save_steps: 100
  save_total_limit: 3  # Keep only 3 best checkpoints
  save_safetensors: true  # Safer format

  # Evaluation
  eval_strategy: steps    # or "epoch"
  eval_steps: 100
  eval_delay: 0
  metric_for_best_model: eval_loss
  greater_is_better: false
  load_best_model_at_end: true

  # Early stopping
  early_stopping: true
  early_stopping_patience: 3
  early_stopping_threshold: 0.001

  # Training optimizations
  group_by_length: true  # Group similar length sequences
  length_column_name: length

  # Data loading
  dataloader_drop_last: false
  dataloader_num_workers: 4
  dataloader_pin_memory: true
  dataloader_persistent_workers: true

  # Seed for reproducibility
  seed: 42
  data_seed: 42

  # Distributed training (if using multiple GPUs)
  ddp_find_unused_parameters: false
  ddp_bucket_cap_mb: 25
  ddp_broadcast_buffers: false

  # Experiment tracking
  report_to: tensorboard    # or "wandb", "mlflow", "none"
  run_name: qwen-promptevolver-qlora

  # Push to hub (optional)
  push_to_hub: false
  hub_model_id: username/qwen-promptevolver
  hub_strategy: every_save
  hub_token:

  # Resume training
  resume_from_checkpoint:       # or path to checkpoint

  # Torch compile (PyTorch 2.0+)
  torch_compile: false  # Enable for potential speedup
  torch_compile_backend: inductor
  torch_compile_mode: default

# Dataset Configuration
dataset:
  # Paths to data splits
  train_path: ./data/processed/splits/train_latest.json
  validation_path: ./data/processed/splits/val_latest.json
  test_path: ./data/processed/splits/test_latest.json

  # Data processing
  max_length: 512  # Maximum sequence length
  truncation: true
  padding: max_length    # or "longest"

  # Prompt template (for instruction tuning)
  prompt_template: |
    ### Instruction:
    Optimize the following prompt to be more effective and specific:
    {original_prompt}

    ### Response:
    {enhanced_prompt}

  # Data augmentation
  augmentation:
    enabled: false
    techniques:
    - paraphrase
    - back_translation
    - synonym_replacement

  # Data filtering
  filter:
    min_quality_score: 0.6
    max_length: 1024
    min_length: 10
    domains: [Analytics, Coding, Content, Cross-Domain]

# Evaluation Configuration
evaluation:
  # Metrics to compute
  metrics:
  - perplexity
  - bleu
  - rouge
  - quality_score

  # Evaluation sampling
  num_samples: 100
  temperature: 0.7
  top_p: 0.9
  top_k: 50

  # Human evaluation (optional)
  human_eval:
    enabled: false
    num_samples: 20
    criteria:
    - clarity
    - specificity
    - effectiveness

# Hardware Configuration
hardware:
  # Device placement
  device_map: auto    # Automatic device mapping

  # Memory management
  max_memory:
    0: 22GB    # GPU 0 max memory (adjust based on your GPU)
    cpu: 50GB      # CPU RAM for offloading

  # Flash Attention 2 (if supported)
  use_flash_attention_2: false  # Enable if hardware supports

  # CPU offload
  offload_folder: ./offload
  offload_state_dict: false

  # Disk offload
  disk_offload_folder: ./disk_offload

# Monitoring Configuration
monitoring:
  # TensorBoard
  tensorboard:
    enabled: true
    log_dir: ./logs/tensorboard
    histogram_freq: 100
    profile_batch: 10,20

  # Weights & Biases (optional)
  wandb:
    enabled: false
    project: promptevolver
    entity: your-entity
    tags: [qwen, qlora, prompt-optimization]
    notes: Fine-tuning Qwen for prompt optimization

  # MLflow (optional)
  mlflow:
    enabled: false
    tracking_uri: http://localhost:5000
    experiment_name: qwen-promptevolver

# Inference Configuration (for testing)
inference:
  # Generation parameters
  max_new_tokens: 256
  temperature: 0.7
  top_p: 0.9
  top_k: 50
  do_sample: true
  num_beams: 1
  repetition_penalty: 1.1

  # Batch inference
  batch_size: 8

  # Model loading
  load_in_8bit: false  # Use 8-bit for inference (more memory efficient)
  load_in_4bit: true   # Use 4-bit for inference (most memory efficient)

# Experiment Tracking
experiment:
  name: PromptEvolver-QLoRA-Training
  version: 1.0.0
  description: Fine-tuning Qwen model for prompt optimization using QLoRA
  tags:
  - qwen
  - qlora
  - prompt-engineering
  - '2025'

  # Hyperparameter search (optional)
  hyperparameter_search:
    enabled: false
    backend: optuna    # or "ray", "wandb"
    n_trials: 20
    parameters:
      learning_rate:
        type: float
        low: 1e-5
        high: 5e-4
        log: true
      lora_r:
        type: categorical
        choices: [8, 16, 32]
      per_device_train_batch_size:
        type: categorical
        choices: [2, 4, 8]

# Validation Checks
validation:
  # Pre-training checks
  check_data_quality: true
  check_model_loading: true
  check_memory_usage: true

  # During training checks
  monitor_loss_spike: true
  monitor_gradient_norm: true
  monitor_learning_rate: true

  # Post-training checks
  evaluate_on_test: true
  generate_samples: true
  save_predictions: true

# Notes and Best Practices
notes: |
  Best Practices for QLoRA Training (2025):

  1. Memory Optimization:
     - Use gradient_checkpointing for large models
     - Enable gradient_accumulation_steps for larger effective batch
     - Use paged_adamw_32bit optimizer
     - Enable group_by_length for efficient batching

  2. Quality vs Speed Trade-offs:
     - Higher LoRA rank (r) = better quality but more memory
     - Target all linear layers for best results
     - Use cosine scheduler for smooth learning

  3. Hardware Recommendations:
     - Minimum 16GB VRAM for 3B model with QLoRA
     - Use Flash Attention 2 if available
     - Enable bf16 on newer GPUs (A100, H100)

  4. Monitoring:
     - Watch for loss spikes or gradient explosions
     - Monitor GPU memory usage
     - Track quality metrics beyond just loss

  5. Data Quality:
     - Ensure balanced domain distribution
     - Filter low-quality samples
     - Use proper prompt templates
