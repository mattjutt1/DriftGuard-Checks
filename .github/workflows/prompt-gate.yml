name: Prompt Gate

on:
  pull_request:
    types: [opened, synchronize, reopened, labeled]
  workflow_dispatch: {}

jobs:
  prompt-check:
    runs-on: ubuntu-latest

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Soft guard (run only if label present)
      run: |
        python - <<'PY'
        import json, os, sys
        ev=json.load(open(os.environ['GITHUB_EVENT_PATH']))
        names=[l['name'] for l in ev['pull_request'].get('labels',[])]
        if 'prompt-check' not in names:
            print('Label prompt-check not present; soft-skip.')
            open('results.json','w').write('{"skipped":true}')
            sys.exit(0)  # neutral
        PY

    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'

    - name: Install PromptOps SDK
      run: |
        cd library/
        pip install -e .

    - name: Run prompt evaluation
      id: eval
      env:
        PROMPTOPS_MODE: stub
        DISABLE_NETWORK: 1
      run: |
        promptops ci --config examples/.promptops.yml --out results.json

    - name: Upload results
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: prompt-evaluation-results
        path: results.json

    - name: Write Prompt Gate summary
      if: always()
      run: |
        python3 - <<'PY'
        import json
        import os
        import sys

        try:
            with open('results.json', 'r') as f:
                results = json.load(f)
        except FileNotFoundError:
            with open(os.environ["GITHUB_STEP_SUMMARY"], "a", encoding="utf-8") as f:
                f.write("### 🔇 Prompt Gate Skipped\n\n")
                f.write("The `prompt-check` label is not present on this PR. Add the label to enable prompt evaluation.\n\n")
                f.write("This workflow runs offline with no API costs.\n")
            sys.exit(0)

        if results.get("skipped", False):
            with open(os.environ["GITHUB_STEP_SUMMARY"], "a", encoding="utf-8") as f:
                f.write("### 🔇 Prompt Gate Skipped\n\n")
                f.write("The `prompt-check` label is not present on this PR. Add the label to enable prompt evaluation.\n\n")
                f.write("This workflow runs offline with no API costs.\n")
            sys.exit(0)

        # Process normal results
        status = "✅ PASSED" if results["pass"] else "❌ FAILED"
        win_rate = results["metrics"]["win_rate"]
        threshold = results["threshold"]
        prompt_count = results["metrics"].get("total_prompts", 0)
        avg_score = results["metrics"].get("avg_score", 0)
        simulated_cost = prompt_count * 0.002

        with open(os.environ["GITHUB_STEP_SUMMARY"], "a", encoding="utf-8") as f:
            f.write("### Prompt Gate Results\n\n")
            f.write(f"- **Status:** {status}\n")
            f.write(f"- **Win Rate:** {win_rate:.1%} (threshold: {threshold:.1%})\n")
            f.write(f"- **Prompts Tested:** {prompt_count}\n")
            f.write(f"- **Average Score:** {avg_score:.3f}\n")
            f.write(f"- **Simulated Cost:** ${simulated_cost:.4f}\n\n")
            f.write("📊 [View detailed results](../actions/runs/" + os.environ["GITHUB_RUN_ID"] + ") • ")
            f.write("📁 [Download artifacts](../actions/runs/" + os.environ["GITHUB_RUN_ID"] + "#artifacts)\n\n")
            f.write("This evaluation ran offline with no actual API costs. ")
            f.write("To enable real LLM providers, see the [setup guide](https://github.com/mattjutt1/prompt-wizard#llm-provider-configuration-offline-by-default).\n")
        PY
