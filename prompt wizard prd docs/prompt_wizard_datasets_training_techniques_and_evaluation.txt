PromptWizard – Datasets, Training Techniques, and Evaluation1. Dataset Structure in PromptWizardPromptWizard expects training data in a simple JSONL format. Each line is a JSON object with two fields: `question` and `answer`. The question is the full input query or problem, and the answer is the ground‑truth response. Both the training and test files use this same format.Example (GSM8K)```json{  "question": "Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?",  "answer": "Natalia sold 48/2 = 24 clips in May. Natalia sold 48+24 = 72 clips altogether in April and May. #### 72"}```PromptWizard stores these files in a `data/` directory as `train.jsonl` and `test.jsonl`. The answer may embed step‑by‑step reasoning; you can set an `answer_format` to help the system extract the final answer.---2. Training Data Sources for Prompt Enhancement| Category                              | Dataset                             | Size     | Format                                  | License                       | Download                                                                                                                                                                                       || ------------------------------------- | ----------------------------------- | -------- | --------------------------------------- | ----------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- || **Instruction‑following**             | **Stanford Alpaca**                 | 52 k     | JSON (`instruction`, `input`, `output`) | CC BY‑NC 4.0 (non‑commercial) | [https://huggingface.co/datasets/tatsu-lab/alpaca](https://huggingface.co/datasets/tatsu-lab/alpaca)                                                                                           ||                                       | **Databricks Dolly 15k**            | 15 k     | JSONL                                   | CC BY‑SA 3.0                  | [https://huggingface.co/datasets/databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)                                                             ||                                       | **OpenAssistant OASST1**            | 161 k    | JSONL (multi‑turn)                      | Apache 2.0                    | [https://huggingface.co/datasets/OpenAssistant/oasst1](https://huggingface.co/datasets/OpenAssistant/oasst1)                                                                                   ||                                       | **WizardLM Evol‑Instruct V2**       | 196 k†   | JSONL                                   | MIT (partial)                 | [https://huggingface.co/datasets/WizardLM/WizardLM\_evol\_instruct\_V2\_196k](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)                                         || **Prompt‑engineering (before/after)** | **Bad‑Improved Prompt Pairs**       | 1.2 k    | CSV / JSON                              | Apache 2.0                    | [https://huggingface.co/datasets/jayveersinh/bad\_improved\_prompts](https://huggingface.co/datasets/jayveersinh/bad_improved_prompts)                                                         ||                                       | **Taron Prompt Optimization**       | 102      | JSON                                    | Unknown                       | [https://huggingface.co/datasets/taronklm/better\_prompt\_optimization](https://huggingface.co/datasets/taronklm/better_prompt_optimization)                                                   ||                                       | **Prompt‑Enhancement Mini (Image)** | 1.1 k    | JSON (`short`, `long`)                  | CreativeML OpenRAIL‑M         | [https://huggingface.co/datasets/fofr/Prompt-Enhancement-Mini](https://huggingface.co/datasets/fofr/Prompt-Enhancement-Mini)                                                                   || **Chain‑of‑thought reasoning**        | **GSM8K**                           | 8.5 k    | JSON (`question`, `answer`)             | MIT                           | [https://huggingface.co/datasets/openai/gsm8k](https://huggingface.co/datasets/openai/gsm8k)                                                                                                   ||                                       | **MATH**                            | 12.5 k   | JSON                                    | MIT                           | [https://huggingface.co/datasets/hendrycks/competition\_math](https://huggingface.co/datasets/hendrycks/competition_math)                                                                      ||                                       | **BIG‑Bench Hard (BBH)**            | 23 tasks | JSON                                    | Apache 2.0                    | [https://github.com/google/BIG-bench](https://github.com/google/BIG-bench)                                                                                                                     ||                                       | **HumanEval**                       | 164      | JSON (Python problems)                  | MIT                           | [https://github.com/openai/human-eval](https://github.com/openai/human-eval)                                                                                                                   || **Task‑specific / meta‑prompt**       | **BIG‑Bench Instruction Induction** | 19 tasks | JSON                                    | Apache 2.0                    | [https://github.com/google/BIG-bench/tree/main/bigbench/benchmark\_tasks/instruction\_induction](https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/instruction_induction) |†WizardLM V2 includes ~143 k MIT‑licensed Alpaca‑derived evolutions plus ~53 k non‑commercial ShareGPT pairs. Use the MIT subset for commercial training.Sample Prompt Transformation (Bad‑Improved Prompt Pairs)| Original                     | Improved                                                                               || ---------------------------- | -------------------------------------------------------------------------------------- || “Help me with C++ libraries” | “Provide a list of the most popular C++ libraries and briefly describe their purpose.” |Sample Context Expansion (Prompt‑Enhancement Mini)| Short                                            | Long                                                                                                                                                                          || ------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- || “Ghostly figure stands in foggy field at night.” | “A ghostly figure stands in the middle of a fog‑covered field under a starless night sky, faint moonlight reflecting off the mist while distant trees form dark silhouettes…” |---3. Key Training Techniques in PromptWizardNo model weight tuning – PromptWizard keeps the LLM fixed.Iterative self‑refinement loop:Mutate the instruction / few‑shot examples.Score each candidate on a small training set (e.g. accuracy on answers).Critique failures using the LLM.Synthesize an improved prompt.Repeats until performance plateaus (often ≤ 5 iterations, ≈ 10‑150 API calls).Acts like a cost‑efficient evolutionary search; no LoRA, RLHF, or gradient updates are used.---4. Evaluation Metrics| Metric                           | How PromptWizard Uses It                                                     || -------------------------------- | ---------------------------------------------------------------------------- || **Task Accuracy / Success Rate** | Percentage of correct answers on held‑out test set (e.g. GSM8K).             || **Exact vs. LLM‑Judged Match**   | For numeric/string answers, direct match; for open answers, LLM judge.       || **Cross‑Task Performance**       | Measures robustness across 40 + tasks (GSM8K, MMLU, BBH, etc.).              || **Cost & Efficiency**            | Counts API calls / tokens to reach final prompt; compares against baselines. || **Few‑Shot Generalization**      | Tests prompts trained with 5 vs. 25 examples to check robustness.            || **Human Readability (optional)** | Manual review of clarity and completeness of optimized prompt.               |PromptWizard often reports large accuracy gains (e.g. GPT‑3.5 on GSM8K: ~25 % → 90 %) with > 10× cost savings compared to other prompt‑search algorithms.---Practical Pipeline for Qwen‑30B Prompt RefinerBase Supervision: Fine‑tune on instruction‑following datasets that allow commercial use (Dolly 15k, OASST1).Specialized Supervision: Fine‑tune (LoRA or full) on prompt‑engineering pairs (Bad‑Improved, Prompt‑Enhancement Mini).Reasoning Boost: Add chain‑of‑thought datasets (GSM8K, MATH) so the model learns to embed reasoning or task decomposition.Evaluation: Measure:Accuracy delta on tasks when using original vs. model‑enhanced prompts.Prompt length / cost trade‑off.Human preference for clarity.Optional External Optimizer: You can still run PromptWizard on top of Qwen‑30B to auto‑refine prompts beyond the model’s own rewrite.---SourcesPromptWizard GitHub README, project blog, and Hugging Face dataset pages (Alpaca, Dolly 15k, Bad‑Improved Prompts, GSM8K, MATH, WizardLM, etc.). All cited licenses verified August 2025.